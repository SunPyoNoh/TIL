# 19/08/28 Spark



Spark?

인메모리 기반의 대용량 데이터 고속 처리 엔진으로 범용 분산 클러스터 컴퓨팅 프레임워크

Spark 구성 요소?
- 클러스터 매니저 : Spark standalone, Yarn, Mesos
- SparkCore 
- Spark SQL
- Spark Streaming - 실시간 처리
- MLlib
- Graph X



Spark에서는 데이터 처리하기 추상화된 모델 : RDD(복구가능한 분산 데이터셋)



SparkApplication 구현 단계 :
1. SparkContext 생성
   - Spark애플리케이션과 Spark 클러스터와의 연결을 담당하는 객체
   - 모든 스파크 애플리케이션은 SparkContext를 이용해 RDD나 accumulator 또는 broadcast 변수 등을 다루게 됩니다.
   - Spark 애플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는 역할을 한다

2. RDD(불변데이터 모델, parition가능 ) 생성
   - collection, HDFS, hive, CSV 등 ...
3. RDD 처리 - 변환연산(RDD에 요소의 구조 변경, filter처리, grouping ...)
4. 집계, 요약 처리 - Action 연산
5. 영속화



SparkApplication => Job

Spark 클러스터 환경에서 node들  : SparkClient, Master노드, Worker노드

SparkClient 역할 - SparkApplication 배포하고 실행을 요청

Spark Master노드 역할 -  Spark 클러스터 환경에서 사용가능한 리소스들의 관리

Spark Worker노드 역할 - 할당받은 리소스(CPU core, memory)를 사용해서  SparkApplication 실행 

Spark Worker노드에서 실행되는 프로세스 - Executor는 RDD의 partition을 task단위로 실행



Spark 장점

1. 반복처리와 연속으로 이루어지는 변환처리를 고속화 (메모리기반)
2. 딥러닝, 머신러닝등의 실행환경에 적합한 환경 제공
3. 서로 다른 실행환경과 구조, 데이터들의 처리에 대해서 통합 환경 제공



```spark
sc.textFile() : file로 부터 RDD생성
collect
map, flatMap()
mkString("구분자")
```